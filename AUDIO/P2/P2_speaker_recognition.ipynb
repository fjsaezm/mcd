{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","celltoolbar":"Slideshow","colab":{"name":"P2_speaker_recognition.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qncY3FktdgMI"},"source":["# APPSA - Práctica 2: Reconocimiento de Locutor en VoxCeleb\n","\n","**Alicia Lozano Díez**\n"," \n","Abril, 2022\n"]},{"cell_type":"markdown","metadata":{"id":"V6yNO58J-7my"},"source":["## Objetivo\n","\n","Explorar y evaluar un sistema de Reconocimiento de Locutor basado en embeddings (x-vectors) sobre la tarea de VoxCeleb.\n","\n","\n","### Materiales\n","\n","- Presentación de la práctica - Moodle\n","- Guión de la práctica - Moodle\n","- Información sobre VoxCeleb Challenge - https://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html\n","- Código de la práctica - Moodle y One Drive:\n","> * https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EeymgCUQ2EhPrAvEWf6SQm8By5753NkZHmvmPMslib0A3A?download=1 \n","- Conjunto de datos de VoxCeleb 1 test para evaluación \n","> * https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/ER9-4DcM62lLtyqMkcU0ZAwBGHMFaSrPZgzuLq0bAxLB-Q?download=1\n","- Subconjunto de datos de Voxceleb 2 dev para entrenamiento \n","> * https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EQkvCJ901zxApjlVbbSeyooBfa7it0iQP2Eie1MIsy3V-Q?download=1\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P1BzhOOn-6H4"},"source":["# 1. Preparación del entorno\n","\n","## 1.1. Preparación del entorno software\n","\n","Primero vamos a descargar el código de la práctica. \n","\n","Para ello, necesitaremos descargar de Moodle el script **code_download_onedrive.sh**, y ejecutar las siguientes líneas de código, que nos permitirán subir el archivo a Google Colab desde el disco local:"]},{"cell_type":"code","metadata":{"id":"p9y6dYqk8Yxe"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YXA2clla9YCh"},"source":["Podemos comprobar la ruta donde estamos y listar los ficheros contenidos en el directorio actual:"]},{"cell_type":"code","metadata":{"id":"RDuzqeTF8gtw"},"source":["!pwd\n","!ls "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTQA1q8V93vQ"},"source":["Le damos permisos de ejecución al script, y descargamos el código de la práctica: "]},{"cell_type":"code","metadata":{"id":"6Q30xT2V7-xI"},"source":["!chmod 755 code_download_onedrive.sh\n","!./code_download_onedrive.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kCktM19-ECu"},"source":["Nos movemos al directorio APPSA_PR2. \n","Podemos comprobar los requisitos software que necesitaremos en el fichero **requirements.txt**.\n","La mayoría ya están integrados en Google Colab."]},{"cell_type":"code","source":["%cd APPSA_PR2"],"metadata":{"id":"ZCYgeRY5AEdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3S8TA_cq4UE"},"source":["!cat requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlRv5IUs_LZv"},"source":["Comprobamos que podemos importar todos los módulos mencionados como requisitos para usar este toolbox. "]},{"cell_type":"code","metadata":{"id":"1LmRwvkIrC2v"},"source":["import torch\n","import torchaudio\n","import numpy\n","import scipy\n","import sklearn\n","import tqdm\n","import yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SBeca0bs_RlD"},"source":["Podemos comprobar las versiones de los módulos de la siguiente manera:"]},{"cell_type":"code","metadata":{"id":"qosX5KN_w9Ax"},"source":["print(torch.__version__)\n","print(torchaudio.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mbi_IL1i1SzF"},"source":["## 1.2. Descarga y descripción de los datos\n","Ahora vamos a descargar los conjuntos de datos preparados para la práctica desde sus enlaces de One Drive. \n","\n","Para descargarlos y descomprimirlos en el directorio que esperan los scripts de entrenamiento, podemos utilizar el script **data_download_onedrive.sh** (previamente descargarlo de Moodle). \n"]},{"cell_type":"code","metadata":{"id":"KRtnlv0sUosl"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b49CexJXOUCl"},"source":["!chmod 755 data_download_onedrive.sh\n","!./data_download_onedrive.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gik4HE4lJQGt"},"source":["!ls data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02uW7QJVRHj4"},"source":["!ls data/voxceleb1/*/* | head"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sY9gFCEkCrVW"},"source":["Como vemos, el script de descarga de datos los ha preparado en la carpeta **data**. Dentro de la misma tenemos: \n","- *voxceleb1*: ficheros de audio WAV correspondientes al conjunto de evaluación (verificación).\n","- *voxceleb2*: ficheros de audio WAV correspondientes al conjunto de entrenamiento. Es un subconjunto del conjunto original VoxCeleb2 dev.\n","\n","Además, tenemos las listas de ficheros que utilizaremos para entrenamiento y evaluación (test) en el directorio **lists**. Dichas listas tienen la siguiente forma: \n","- *train_list.txt*: SPKID    WAVFILE\n","- *veri_test.txt*:  KEY(0 - non target, 1 - target)   SPKMODEL_FILE    TEST_FILE\n","\n","Podemos ver unas cuantas líneas de cada fichero:"]},{"cell_type":"code","metadata":{"id":"CKn9b5KvD9Tm"},"source":["!head lists/train_list.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwzJDXv2EBMu"},"source":["!head lists/veri_test.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyPCiZUOChpy"},"source":["## 1.3. Descripción del sistema \n","El sistema utilizado en esta práctica consiste de un extractor de embeddings (x-vectors), cuyo objetivo es extraer una representación del fichero de audio de longitud fija que contenga información distriminativa para la tarea de reconocimiento de locutor. Este extractor de x-vectors puede diseñarse con diferentes arquitecturas de redes neuronales y se puede entrenar con diferentes funciones de coste, así como diferentes parámetros de configuración.\n","\n","Los scripts de entrenamiento del extractor de x-vectors principales son:\n","- *trainSpeakerNet.py*: es el script principal para entrenar o evaluar (un modelo previamente entrenado) el extractor de x-vectors. Recibe diferentes valores para sus argumentos con los que se determina el comportamiento del script.\n","- *models/**: en esta carpeta se encuentra la definición de diferentes arquitecturas que se pueden emplear como extractores de x-vectors.\n","- *loss/**: este directorio contiene la definición de diferentes funciones de coste que se pueden utilizar como funciones objetivo durante el entrenamiento del modelo.\n","\n","Además, para realizar un análisis del rendimiento de los diferentes modelos, hemos incluido el siguiente script:\n","- *plot_score_histograms.py*: este script genera los histogramas de puntuaciones target y non-target de un modelo previamente evaluado."]},{"cell_type":"markdown","metadata":{"id":"HpX4vvEIGQWj"},"source":["# 2. Identificación de líneas claves de código\n","\n","Ya que como se ha mencionado previamente, el script **trainSpeakerNet.py** es el principal, vamos explorar el código de dicho script para entender el funcionamiento del sistema.\n","\n","**PREGUNTAS:** Incluye las respuestas a estas preguntas en la memoria de la práctica: \n","- ¿Qué línea de código se corresponde con el entrenamiento del modelo?\n","- ¿Qué comando carga un modelo previamente entrenado?\n","- ¿Qué código evalúa el rendimiento de la red neuronal?\n","- ¿Qué variable contiene las puntuaciones (scores) de la lista de trials? ¿En qué fichero se guardarán dichas puntuaciones?\n","- ¿Qué argumento controla el número de épocas (iteraciones) que se entrena el modelo?\n","- ¿Qué parámetro determina la función de coste?\n","- ¿Para qué se usa el parámetro *test_interval*?\n","- ¿Qué argumento necesitarías modificar para cambiar el tipo (arquitectura) del modelo que quieres entrenar?\n","- ¿Para qué se utiliza el parámetro *nClasses*? ¿Cómo obtendrías este valor a partir de tu conjunto de datos? Escribe el comando que utilizarías (por ejemplo, en Linux)."]},{"cell_type":"markdown","metadata":{"id":"cIu5CAHYAX3R"},"source":["# 3. Evaluación de modelos preentrenados \n","En el directorio **pretrained_models/** podemos encontrar dos modelos previamente entrenados con la base de datos VoxCeleb2 dev (partición de desarrollo). \n","\n","Para evaluar cada uno de estos modelos previamente entrenados, podemos utilizar los siguientes comandos:\n"]},{"cell_type":"code","metadata":{"id":"k5ZYPxQM9N5s"},"source":["!python ./trainSpeakerNet.py --eval --model ResNetSE34L --log_input True \\\n","          --trainfunc angleproto --save_path exps/test_lite --eval_frames 400 \\\n","          --test_list lists/veri_test.txt \\\n","          --initial_model pretrained_models/baseline_lite_ap.model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcXugrL3y-3W"},"source":["!python ./trainSpeakerNet.py --eval --model ResNetSE34V2 --log_input True \\\n","  --encoder_type ASP --n_mels 64 --trainfunc softmaxproto \\\n","  --save_path exps/test_v2 --eval_frames 400 --test_list lists/veri_test.txt \\\n","  --initial_model pretrained_models/baseline_v2_ap.model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RHprX8-Bswg"},"source":["**PREGUNTAS:** Incluye en la memoria de la práctica las respuestas a las siguientes preguntas:\n","- ¿Cuál es el rendimiento de cada uno de los modelos?\n","- ¿Cuál es mejor? ¿Por qué?\n","- ¿Qué es el EER? ¿Cómo podemos interpretarlo?\n","- ¿Cuáles son las diferencias entre ambos modelos?"]},{"cell_type":"markdown","metadata":{"id":"TzxVJ9-CVykS"},"source":["# 4. Entrenamiento de un modelo\n","Utilizando el script **trainSpeakerNet.py**, entrena dos extractores de embeddings (x-vectors) a partir del subconjunto dado en **data/voxceleb2**. Ten en cuenta que el entrenamiento puede llevar bastante tiempo dependiendo del hardware disponible. Puedes parar el entrenamiento fijando un número máximo de iteraciones (épocas).\n","\n","## 4.1. Entrenamiento de un modelo con una configuración fija\n","\n","Entrena el siguiente modelo: ResNetSE34L, SAP encoder, tamaño de batch de 200, función de coste amsoftmax (con escalado 30 y margen 0.3). También necesitarás el número de locutores que contiene el subconjunto de entrenamiento dado para especificar el número de clases, así como las rutas a los conjuntos y listas de entrenamiento y evaluación.\n","\n","Una vez que el entrenamiento ha finalizado, analiza los resultados sobre el conjunto de evaluación **data/voxceleb1**, representando los histogramas de scores correspondientes (**plot_score_histograms.py**). "]},{"cell_type":"code","metadata":{"id":"vqbOau7ZWBH1"},"source":["# plot_score_histograms.py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","scores_file = 'exps/test_lite/result/scores_test.txt' # Modify to point at your scores file \n","keys_file = 'lists/veri_test.txt'\n","\n","all_scores = np.loadtxt(scores_file, usecols=[0])\n","all_trials_scores = np.loadtxt(scores_file, usecols=[1,2], dtype='str')\n","\n","all_keys = np.loadtxt(keys_file, usecols=[0])\n","all_trials_keys = np.loadtxt(keys_file, usecols=[1,2], dtype='str')\n","\n","\n","target_scores = all_scores[all_keys==1]\n","nontarget_scores = all_scores[all_keys==0]\n","\n","\n","plt.hist(target_scores, alpha=0.7)\n","plt.hist(nontarget_scores, alpha=0.7)\n","\n","plt.xlabel('score')\n","plt.legend(('target','non target')) \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mo-xgJFXdXD"},"source":["**PREGUNTAS:** Responde a las siguientes preguntas:\n","- ¿Cuántas épocas has usado?\n","- ¿Cuál es el rendimiento del modelo?\n","- ¿Cómo es este rendimiento en comparación con el analizado en el apartado anterior de los modelos preentrenados? ¿Por qué piensas que ocurre esto?\n","- Incluye los histogramas de tu sistema en la memoria de la práctica. ¿Qué umbral elegirías (valor aproximado) para clasificar entre target y non-target trials? ¿Por qué?"]},{"cell_type":"markdown","metadata":{"id":"_kgeuI5_JVS-"},"source":["\n","## 4.2. OPCIONAL: Entrenamiento de un modelo a tu elección\n","\n","Ahora, selecciona un modelo de aquellos definidos en **models/** así como una función de coste de las disponibles en la carpeta **loss/**. Los argumentos que necesitarás para entrenar dicho modelo dependerán de la configuración elegida.\n","\n","**PREGUNTAS:** Contesta las siguientes preguntas en la memoria de la práctica:\n","- ¿Qué comando (modelo, parámetros, etc.) has utilizado?\n","- ¿Cuál es el rendimiento del sistema?\n","- ¿Cómo es el rendimiento de este sistema con respecto al resto de modelos de la práctica? ¿Por qué?\n","\n","Por último, entrena este mismo modelo pero en una configuración con menos parámetros (modelo más pequeño) y evalúalo. \n","\n","**PREGUNTAS:** Incluye las respuestas a estas preguntas en tu informe de la práctica: \n","- ¿Cómo es el rendimiento de este modelo pequeño con respecto al grande? ¿Por qué?\n","- Dibuja el histograma de scores para ambos modelos (pequeño y grande)."]}]}